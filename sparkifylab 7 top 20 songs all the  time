from pyspark import SparkContext
import json
import time
# Creating a spark context with 4 threads
sc = SparkContext('local[4]', 'top20Songs')
sc.setLogLevel('FATAL')
start = time.perf_counter()
input_bucket = "s3://dsci6007yshah1/"
#input_bucket = "s3://yesha.shah.dsci6007.bucket/sample/"
# Reading all JSON objects from s3 bucket
obj = sc.wholeTextFiles(input_bucket)
# Since wholeTextFiles has Rows as (file, content), mapping done only on 
# values (i.e. content)
# Then, discarding file names bu mapping only content
counter = obj.mapValues(lambda event : (json.loads(event)["song"], 1) if
json.loads(event)["song"] else (None, 0)).map(lambda x : x[1])
# Using .reduceByKey to aggregate all (artist name, 1) to 
#(artist name, count)
# .take() automatically limits and collects to return a list
counter = counter.reduceByKey(lambda a,b:a +b).sortBy(lambda a : -
a[1]).take(20)
print('Time taken for entire job = {:.4f}s'.format(time.perf_counter() -
start))
print('\nThe top 20 songs are:\nArtist\t\t\tCount\n')

# Storing results to a .txt file
with open('songs.txt', 'w') as f:
 f.write("Song\t\tCount")
 for (song, count) in counter:
 print(song,'\t\t',count)
 f.write('{}\t\t{}\n'.format(song, count))
f.close()

